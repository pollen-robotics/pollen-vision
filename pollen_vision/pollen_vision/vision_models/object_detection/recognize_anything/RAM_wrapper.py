import json
import os
from typing import List

import cv2
import numpy as np
import numpy.typing as npt
import torch
from PIL import Image
from pollen_vision.vision_models.utils import get_checkpoint_path, get_checkpoints_names
from ram import get_transform
from ram import inference_ram_openset as inference
from ram.models import ram_plus
from ram.utils import build_openset_llm_label_embedding
from torch import nn

IMAGE_SIZE: int = 384


class RAM_wrapper:
    """A wrapper for the Recognize Anything Model (RAM)."""

    def __init__(
        self,
        objects_descriptions_filename: str,
        objects_descriptions_folder_path: str = f"{os.path.dirname(__file__)}/objects_descriptions",
        checkpoint_name: str = "ram_plus_swin_large_14m",
    ) -> None:
        """
        Args:
            - objects_descriptions_filename: the name of the json file containing the descriptions of the objects.
                Descriptions files are generated by generate_objects_descriptions.py
            - objects_descriptions_folder_path: path to the folder containing the descriptions of the objects.
                Defaults to "objects_descriptions" in the same directory as this file.
            - checkpoint_name: the name of the checkpoint to use. Defaults to "ram_plus_swin_large_14m".
        """
        valid_names = get_checkpoints_names()
        if checkpoint_name not in valid_names:
            raise ValueError(f"Could not find the model checkpoint {checkpoint_name}. Valid checkpoints are: {valid_names}")

        self._checkpoint_path = get_checkpoint_path(checkpoint_name)

        try:
            object_description = json.load(
                open(f"{objects_descriptions_folder_path}/{objects_descriptions_filename}.json", "rb")
            )
        except FileNotFoundError:
            raise FileNotFoundError(
                f"Could not find the file {objects_descriptions_folder_path}/{objects_descriptions_filename}.json"
            )

        # Building embeddings
        self._openset_label_embedding, self._openset_categories = build_openset_llm_label_embedding(object_description)
        self.open_set_categories = self._openset_categories
        self._device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self._transform = get_transform(image_size=IMAGE_SIZE)
        self._model = ram_plus(pretrained=self._checkpoint_path, image_size=IMAGE_SIZE, vit="swin_l")

        self._model.tag_list = np.array(self._openset_categories)
        self._model.label_embed = nn.Parameter(self._openset_label_embedding.float())
        self._model.num_class = len(self._openset_categories)

        self._model.class_threshold = torch.ones(self._model.num_class) * 0.5
        self._model.eval()
        self._model = self._model.to(self._device)

    def infer(self, im: npt.NDArray[np.uint8]) -> List[str]:
        """Returns a list of labels found in the input image."""
        im = cv2.resize(im, (IMAGE_SIZE, IMAGE_SIZE))
        im = Image.fromarray(im)
        im = self._transform(im).unsqueeze(0).to(self._device)
        res: str = inference(im, self._model)

        labels: List[str] = res.split("|")
        for i in range(len(labels)):
            labels[i] = labels[i].strip()

        return labels
