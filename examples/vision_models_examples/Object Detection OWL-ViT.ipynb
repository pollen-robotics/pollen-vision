{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9052f047-0ed1-435f-9ab8-b37b5c22558f",
   "metadata": {},
   "source": [
    "# Perform object detection with pollen-vision\n",
    "\n",
    "Learn how to perform zero shot object detection with the pollen-vision library, using the OWL-ViT model.\n",
    "\n",
    "This notebook will show you how to use our wrapper for the OWL-ViT object detection model developped by the Google Research lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915566f0-7009-4f03-899c-92491831aad8",
   "metadata": {},
   "source": [
    "![Object detection from Reachy's egocentric view](gif/reachy_kitchen_detection.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11993d6-d4d2-4991-b9aa-1210d6c5141d",
   "metadata": {},
   "source": [
    "## A word on OWL-ViT\n",
    "OWL-ViT stands for Vision Transformer for Open-World Localization. It is a zero shot object detection model, meaning the model is able to perform object detection based on text queries, without needing to retrain the model on any labeled data, as it is the case with traditional Deep Learning object detection models.\n",
    "\n",
    "You can find more information on the model on the dedicated page of the [Hugging Face documentation](https://huggingface.co/docs/transformers/model_doc/owlvit). The implementation of the wrapper actually uses Hugging Face's [transformers library](https://huggingface.co/docs/transformers/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d007292-0b1f-41db-899a-6bcbf1a65449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "\n",
    "from pollen_vision.vision_models.object_detection import OwlVitWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e4188-581f-479d-8594-136226a75d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_wrapper = OwlVitWrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e449d38-8cf3-42c6-a76d-4c6b83bedce5",
   "metadata": {},
   "source": [
    "Import the image you want to perform the inference on. \n",
    "\n",
    "Here we will take one of the test image of the project. We placed the demo images and videos in the imagesfolder. Feel free to import your own image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579895a4-d391-41b7-b18c-1f8d937c8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('images/reachy_living_room.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afac224-c67c-4258-b0b1-a5a02332249e",
   "metadata": {},
   "source": [
    "## Run inference with the model\n",
    "\n",
    "As explained, the OWL-ViT model is a zero shot object detection model and takes text queries as input. The inference is performed with the *infer* method. Just pass as argument a list of the candidate for the object detection that you want to detect. OWL-ViT will only try to detect classes that are in the list.\n",
    "\n",
    "NB: Please note that the image passed as argument for the *infer* method must be a **numpy array object**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766be514-3453-4b0e-8baa-f2315e39eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = object_detection_wrapper.infer(im=np.array(img), candidate_labels=[\"robot\", \"tv\", \"couch\"], detection_threshold=0.15)\n",
    "\n",
    "for prediction in predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3238f284-6683-4d33-9bd2-efb4278197cd",
   "metadata": {},
   "source": [
    "Change the candidates list and check what you can detect!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a3b13-fd0c-47cd-96c0-0b07c1301cde",
   "metadata": {},
   "source": [
    "### Visualize detection results\n",
    "\n",
    "You can visualize easliy the predictions of the model with the *Annotator* class from utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e164bd-1aa5-4764-bbf1-dcd7fa995e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pollen_vision.vision_models.utils import Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf2daf-b161-41f1-a279-21eab2c5b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = Annotator()\n",
    "\n",
    "img_annotated = annotator.annotate(im=np.array(img), detection_predictions=predictions)\n",
    "Image.fromarray(img_annotated)  # annotator returns a numpy array object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee6979f-6854-4697-8246-405010692d8e",
   "metadata": {},
   "source": [
    "## Plug depth camera\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566585ab-b977-499b-8eee-e97d6da5604a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e3eec07-521f-4301-9757-9f3a89710e8b",
   "metadata": {},
   "source": [
    "## Final notes\n",
    "\n",
    "That's all folks! You can use this script if you want to perform zero shot object detection on video frames. The scripts gathers every commands that you saw here in the notebook.\n",
    "\n",
    "Check out the other notebooks if you want to learn how to use other vision models like RAM to check whether an object is in the frame or not or SAM to perform object segmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
