{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09dd9c76-110f-42e7-9f7c-21930dfcc05f",
   "metadata": {},
   "source": [
    "# Perform image tagging with pollen-vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09829964-6c05-4a8e-b480-248bd51c9348",
   "metadata": {},
   "source": [
    "Learn how to perform image tagging with the pollen-vision library, using RAM++.\n",
    "\n",
    "This notebook will show you how to use our wrapper for the RAM++ image tagging model developped by Xinyu Huang et al. at the Oppo research institute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8cf76a-5985-41fa-b96f-a908fdc6d187",
   "metadata": {},
   "source": [
    "## A word on RAM++\n",
    "\n",
    "RAM stands for Recognize Anything Model. RAM is an image tagging model which can recognize any common category of object with high accuracy. RAM++ is the newest generation of RAM which can now also perform zero shot image tagging. This means that the model is able to tag images with any object, considering you provide it with a description of the object.\n",
    "\n",
    "This is very useful for us in Robotics because we can include this in applications to adapt the robot behavior depending of its environment. For example if we ask the robot to try to grasp a mug, it can first check whether there is a mug to grasp or not and if not perform another behavior.\n",
    "\n",
    "You can find the RAM++ paper [here](https://arxiv.org/abs/2310.15200). Our wrapper for RAM uses [its implementation](https://github.com/xinyu1205/recognize-anything?tab=readme-ov-file) for the authors of the paper, credits to them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1760c6b-1895-4d9a-8211-ed7db30e42e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup environment\n",
    "\n",
    "> Note: If you are working locally on your machine and have already installed the library from source, discard the following.\n",
    "\n",
    "We need to first install the pollen-vision library. We will install the library from source, this might take a couple of minutes as there are quite heavy dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eb0027-217f-4281-9088-bd1e4a6d33cf",
   "metadata": {},
   "source": [
    "> If you are on Colab and a warning window pops up indicating \"You must restart the runtime in order to use the newly installed versions.\" don't worry. Just press restart session and don't execute the pip install cell again, pollen-vision will already be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25d6c0-edd5-40f9-a841-d78af143b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"pollen-vision[vision] @ git+https://github.com/pollen-robotics/pollen-vision.git@main\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f226d7a-9569-431c-ae70-9f415af7ec36",
   "metadata": {},
   "source": [
    "To use RAM in Colab, we need a few more steps:\n",
    "\n",
    "\n",
    "*   download the weight of the model\n",
    "*   download the configuration file needed by the model with the description of a few objects to identify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3903b6cd-6ade-492e-a177-9e78b95d2935",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/xinyu1205/recognize-anything-plus-model/resolve/main/ram_plus_swin_large_14m.pth\n",
    "\n",
    "!wget https://raw.githubusercontent.com/pollen-robotics/pollen-vision/develop/pollen_vision/pollen_vision/vision_models/object_detection/recognize_anything/objects_descriptions/example_objects_descriptions.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ad699-5fb9-4b56-a24d-ba786a57324d",
   "metadata": {},
   "source": [
    "Move what has been downloaded to the correct locations and the setup is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a45ae3-5176-4124-932e-96364b10de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ram_plus_swin_large_14m.pth /usr/local/lib/python3.10/dist-packages/checkpoints\n",
    "\n",
    "!mkdir -p /usr/local/lib/python3.10/dist-packages/pollen_vision/vision_models/object_detection/recognize_anything/objects_descriptions\n",
    "\n",
    "!mv example_objects_descriptions.json /usr/local/lib/python3.10/dist-packages/pollen_vision/vision_models/object_detection/recognize_anything/objects_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee165e-2ca1-4a02-a307-b7456069bcfa",
   "metadata": {},
   "source": [
    "## Use the RAM wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4111f7e3-afa0-4958-ac51-12795e766e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from pollen_vision.vision_models.object_detection import RAM_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee979c-01cf-41cd-bd3e-491f3ed2d77b",
   "metadata": {},
   "source": [
    "To use pollen-vision's RAM wrapper, you just need to provide a description file. We provided a description file with a few basic objects if you want to try with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec770db-c60f-4dad-9744-86365a88ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = RAM_wrapper(objects_descriptions_filename=\"example_objects_descriptions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c2f60-8305-4530-ae03-a87d1ee5d993",
   "metadata": {},
   "source": [
    "## Import example image\n",
    "\n",
    "Here we will take one test image of the project where Reachy tries to serve a croissant, a French pastry made from puff pastry in a crescent shape, yummy! ü•ê\n",
    "\n",
    " We will use an image from the [reachy-doing-things image dataset](https://huggingface.co/datasets/pollen-robotics/reachy-doing-things) available on Hugging Face. In this dataset, we captured images from an egocentric view of Reachy doing manipulation tasks while being teleoperated.\n",
    "\n",
    "Feel fry to try the object detection with your own image instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf125849-f828-4af6-9c72-4d1afbab666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"pollen-robotics/reachy-doing-things\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6cd152-ea1b-4f0d-9d11-99dfe0d973ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = dataset[0][\"image\"]\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e9b0f-3a4a-43af-9054-4e5d602fa386",
   "metadata": {},
   "source": [
    "The object classes that RAM can tag with the configuration file that you provided at the instanciation of the master can be checked with the *open_set_categories* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2703c0-5188-4ca0-b8b4-6934894516dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wrapper.open_set_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd43f69-e2fe-4ec3-bc3c-55606bd8ba4c",
   "metadata": {},
   "source": [
    "Let's run RAM to check what objects in its open set it can tag in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b43e5-7288-4164-9c1f-b793ef98528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper.infer(np.array(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd83429-988d-4281-a35d-a0aedc6f7725",
   "metadata": {},
   "source": [
    "So here, based on the objects our current RAM wrapper can tag, only two objects are considered to be in the frame: a humanoid robot and some chairs. But what if we want our wrapper to tag whether a croissant is in the image or not (to start a croissant grasping task for example)? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c9a41-b7c4-4475-8887-c6770e57723f",
   "metadata": {},
   "source": [
    "## Generate a new description file\n",
    "\n",
    "You can easily generate a new description file using pollen-vision with other objects that you want to tag. This file can then be used by the RAM wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b388223d-ca43-44a0-bc7c-48934f32001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pollen_vision.vision_models.object_detection import ObjectDescriptionGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725703e-3c73-4712-9e95-cb46763ba13c",
   "metadata": {},
   "source": [
    "üí° Please note that you will need an OpenAI API key for this as the generator uses the GPT 3.5 model to perform the generation.\n",
    "By default the ObjectDetectionGenerator object looks at init for the API key defined with the **OPENAI_API_KEY** environment variable. If you prefer, you can just pass your API key as argument of the init with the *api_key* argument.\n",
    "\n",
    "If you're working on Colab, you will need to pass your OPENAI api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3aa283-addf-43a6-8a8c-ca8ed5214b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"  # Add your OPENAI API key here\n",
    "\n",
    "if OPENAI_API_KEY != \"\":\n",
    "  object_detection_generator = ObjectDescriptionGenerator(api_key=OPENAI_API_KEY)\n",
    "else:\n",
    "  object_detection_generator = ObjectDescriptionGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e4329-00d9-4704-93ce-beaf490a6fca",
   "metadata": {},
   "source": [
    "Just call the *generate_descriptions* method to generate the discription for the objects you want. Pass a list of objects names as argument. Here we will ask to generate descriptions for our croissant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36c285-8349-40d2-9565-eff183e39ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_objects_list = [\"croissant\"]\n",
    "\n",
    "objects_descriptions = object_detection_generator.generate_descriptions(objects=new_objects_list, generation_nb_per_object=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25afda30-d1c9-4c2c-b122-6fb7294da087",
   "metadata": {},
   "source": [
    "By default, 10 descriptions are generated per object. This can be changed with the optional argument *generation_nb_per_object* of the *generate_descriptions* method. Because a croissant can be a bit technical to tag, we asked for more description per object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37c88d-8d89-4d8e-9c6d-5070eb810783",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(objects_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5743c28-e101-4d25-8d34-679d472e3ccb",
   "metadata": {},
   "source": [
    "You can save the descriptions you just generated to a json file that you can later use with RAM using the *save_descriptions* method. The description file will be saved in the object_descriptions folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa8cda-2c6f-42be-893a-fce8a7141214",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detection_generator.save_descriptions(descriptions=objects_descriptions, descriptor_file_name=\"croissant-descriptor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a3a1ef-a0ab-4030-b423-12115c3e0aa0",
   "metadata": {},
   "source": [
    "You can then use the description file you just generated with RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b76fc8-b42f-47ec-b942-6180b04c6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_ram_wrapper = RAM_wrapper(objects_descriptions_filename=\"croissant-descriptor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be820b-b4d8-4b8d-bee2-5576e9dcf48b",
   "metadata": {},
   "source": [
    "Let's check the objects our new wrapper is able to tag, to make sure if it can tag croissants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077965da-b9cd-4cce-aa16-1af2335efdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_ram_wrapper.open_set_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba486c-0f9b-4abb-9d4e-c7114d7d4b63",
   "metadata": {},
   "source": [
    "Let's perform the image tagging with the same image as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c477475-490c-4da3-a399-ab093d4ee4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_ram_wrapper.infer(np.array(img))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44839291-dff5-4ca4-a7b8-58612fcc1864",
   "metadata": {},
   "source": [
    "Yes, apparently there is a croissant in the image!\n",
    "\n",
    "Let's try to tag on a another image where there is no croissant, just to check if the tag actually works and does not tag a croissant on any image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3ede4-3032-4c0e-8038-7d8f9d90e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_croissant_img = dataset[21][\"image\"]\n",
    "non_croissant_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c62bb0-63ab-4472-9c0a-edbe358fc473",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_ram_wrapper.infer(np.array(non_croissant_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34beed57-56a8-40e7-8a9b-554d98e8a65d",
   "metadata": {},
   "source": [
    "Nice, the croissant tagging seems to work!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
